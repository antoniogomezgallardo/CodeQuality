# Glossary of Software Quality Terms

## A

**Acceptance Criteria**: Specific conditions that must be met for a user story to be considered complete and acceptable to the product owner.

**Agile**: An iterative approach to software development that emphasizes collaboration, customer feedback, and small, rapid releases.

**AI-Assisted Testing**: The use of artificial intelligence and machine learning to enhance testing processes, including test generation, defect prediction, and test optimization.

**Anomaly Detection**: The identification of unusual patterns or behaviors in data that do not conform to expected behavior, often used for detecting defects or performance issues.

**API (Application Programming Interface)**: A set of protocols, routines, and tools for building software applications that specify how software components should interact.

**Automated Testing**: The practice of using software tools to execute tests automatically, compare actual outcomes with predicted outcomes, and report results.

## B

**BDD (Behavior-Driven Development)**: A software development approach that combines Test-Driven Development (TDD) with ideas from Domain-Driven Design and object-oriented analysis.

**Bug**: An error, flaw, or fault in a computer program or system that causes it to produce an incorrect or unexpected result.

**Build**: The process of converting source code files into standalone software artifacts that can be run on a computer.

**Burndown Chart**: A graphical representation of work left to do versus time, showing the progress of a team toward completing a sprint or project.

## C

**Chainof-Thought Prompting**: A prompting technique where you ask the LLM to show its reasoning step-by-step before providing the final answer, improving accuracy for complex tasks.

**ChatGPT**: An AI chatbot developed by OpenAI that uses large language models to generate human-like text responses.

**CI/CD (Continuous Integration/Continuous Deployment)**: A method to frequently deliver apps to customers by introducing automation into the stages of app development.

**Claude**: An AI assistant developed by Anthropic that emphasizes safety, accuracy, and helpful responses.

**Code Coverage**: A measure of how much source code is tested by a particular test suite, usually expressed as a percentage.

**Code Review**: A systematic examination of computer source code intended to find bugs and improve code quality.

**Context Window**: The maximum number of tokens (words/subwords) an LLM can process in a single request, including both input and output.

**Continuous Integration**: The practice of merging all developer working copies to a shared mainline several times a day.

**Copilot**: GitHub's AI pair programmer that suggests code and entire functions in real-time based on context.

**Cyclomatic Complexity**: A software metric used to indicate the complexity of a program by measuring the number of linearly independent paths through source code.

## D

**Defect Prediction**: Using machine learning models to predict which code changes are likely to contain bugs based on historical data and code metrics.

**Definition of Done (DoD)**: A shared understanding of expectations that software must live up to in order to be releasable into production.

**Definition of Ready (DoR)**: A shared understanding of what makes a user story ready for development to begin.

**DORA Metrics**: Four key metrics for measuring DevOps performance: Deployment Frequency, Lead Time for Changes, Mean Time to Recovery, and Change Failure Rate.

**Deployment**: The process of installing, configuring, and enabling a specific application or set of applications.

## E

**Embeddings**: Vector representations of text or code that capture semantic meaning, used in RAG systems and similarity search.

**End-to-End Testing (E2E)**: A methodology that tests an application's workflow from beginning to end to ensure the application works as expected.

**Epic**: A large body of work that can be broken down into a number of smaller stories or requirements.

**Estimation**: The process of predicting the effort required to implement a user story or feature.

## F

**Feature Flag**: A software development technique that turns certain functionality on and off during runtime, without deploying new code.

**Few-Shot Learning**: A technique where an LLM learns from just a few examples provided in the prompt, improving output quality for specific tasks.

**Fine-Tuning**: The process of further training a pre-trained LLM on specific data to adapt it for particular tasks or domains.

**Functional Testing**: Testing that validates the software system against the functional requirements/specifications.

## G

**Generative AI**: AI systems that can create new content (text, code, images) based on patterns learned from training data.

**GitFlow**: A branching model for Git that defines a strict branching structure designed around the project release.

**Given-When-Then**: A style of representing tests or acceptance criteria in a structured format for Behavior-Driven Development.

**GPT (Generative Pre-trained Transformer)**: A type of large language model architecture developed by OpenAI, used in ChatGPT and other AI tools.

## I

**Integration Testing**: Testing performed to expose defects in the interfaces and interaction between integrated components.

**INVEST**: An acronym that defines criteria for good user stories: Independent, Negotiable, Valuable, Estimable, Small, Testable.

**Issue Tracking**: The process of tracking and managing software bugs, feature requests, and other issues throughout development.

## L

**LangChain**: A framework for developing applications powered by language models, providing tools for building RAG systems and AI agents.

**Large Language Model (LLM)**: A type of AI model trained on vast amounts of text data that can understand and generate human-like text.

**Lead Time**: The time elapsed between when a request is made and when it is fulfilled.

**Load Testing**: Testing conducted to understand the behavior of the system under a specific expected load.

**LLaMA**: A family of open-source large language models developed by Meta AI.

## M

**Machine Learning (ML)**: A subset of AI that enables systems to learn and improve from experience without being explicitly programmed.

**Mean Time to Recovery (MTTR)**: The average time required to repair a failed component or device.

**Metrics**: Quantitative measures used to track and assess the status of specific processes.

**Minimum Viable Product (MVP)**: A development technique where a new product is developed with sufficient features to satisfy early adopters.

**Mock**: A simulated object that mimics the behavior of real objects in controlled ways for testing purposes.

## N

**Natural Language Processing (NLP)**: A branch of AI that helps computers understand, interpret, and generate human language.

**Non-Functional Requirements**: Requirements that specify criteria for the operation of a system, rather than specific behaviors.

## P

**Pair Programming**: A software development technique where two programmers work together at one workstation.

**Performance Testing**: Testing conducted to evaluate the speed, responsiveness, and stability of a computer system under a workload.

**Predictive Analytics**: Using statistical algorithms and machine learning to identify the likelihood of future outcomes based on historical data.

**Product Backlog**: An ordered list of features, functions, requirements, enhancements, and fixes that serves as the input for the sprint backlog.

**Prompt Engineering**: The practice of designing effective prompts to get desired outputs from large language models.

## Q

**Quality Assurance (QA)**: A way of preventing mistakes and defects in manufactured products and avoiding problems when delivering products or services.

**Quality Gate**: A checkpoint in the software development process where quality criteria must be met before proceeding.

## R

**RAG (Retrieval-Augmented Generation)**: A technique that combines information retrieval with language model generation to provide more accurate and grounded responses.

**Refactoring**: The process of restructuring existing computer code without changing its external behavior.

**Regression Testing**: Re-running functional and non-functional tests to ensure that previously developed and tested software still performs correctly.

**Requirements Engineering**: The process of defining, documenting, and maintaining requirements in the engineering design process.

**Risk-Based Testing**: An approach to testing that prioritizes the testing of features and functions based on the risk of failure.

## S

**Scrum**: An agile framework for managing product development with an emphasis on software development.

**Security Testing**: Testing technique to determine whether a system protects data and maintains functionality as intended.

**Self-Healing Tests**: Tests that use AI to automatically adapt to UI changes, reducing maintenance overhead.

**Semantic Search**: Search that understands the meaning and context of queries rather than just matching keywords, powered by embeddings.

**Shift-Left**: The practice of moving testing, quality, and performance evaluation early in the development process.

**Shift-Right**: The practice of running tests in production-like environments to ensure quality in real-world conditions.

**SOLID Principles**: Five design principles intended to make software designs more understandable, flexible, and maintainable.

**Sprint**: A time-boxed iteration of a continuous development cycle within an Agile development methodology.

**Static Analysis**: Analysis of computer software performed without actually executing programs.

**Story Points**: A unit of measure for expressing an estimate of the overall effort required to fully implement a product backlog item.

## T

**Technical Debt**: The implied cost of additional rework caused by choosing an easy solution now instead of using a better approach.

**Temperature (LLM)**: A parameter that controls randomness in LLM outputs; lower values (0-0.3) produce more deterministic results, higher values (0.7-1.0) increase creativity.

**Test-Driven Development (TDD)**: A software development process that relies on the repetition of a very short development cycle.

**Test Automation**: The use of software separate from the software being tested to control the execution of tests.

**Test Case**: A set of conditions or variables under which a tester will determine whether a system under test satisfies requirements.

**Test Coverage**: A measure used to describe the degree to which the source code of a program is executed when a particular test suite runs.

**Tokens**: The basic units of text that LLMs process; roughly equivalent to words or subwords (e.g., "testing" = 1 token, "anti-pattern" = 2-3 tokens).

**Traceability**: The ability to trace requirements forward to design and code and backward from code to requirements.

**Transfer Learning**: Using a model trained on one task as the starting point for a different but related task, common in AI/ML.

## U

**Unit Testing**: Testing of individual components or modules of a software application in isolation.

**User Acceptance Testing (UAT)**: Testing performed by end users to verify that a software system meets business requirements.

**User Story**: An informal, natural language description of features of a software system written from the perspective of an end user.

**Usability Testing**: Testing method to evaluate how easy user interfaces are to use.

## V

**Vector Database**: A specialized database optimized for storing and querying vector embeddings, used in RAG systems (e.g., Chroma, Pinecone, Weaviate).

**Velocity**: A measure of the amount of work a team can tackle during a single sprint.

**Version Control**: A system that records changes to a file or set of files over time so that you can recall specific versions later.

**Vulnerability**: A weakness in a system that can be exploited by a threat to gain unauthorized access or perform unauthorized actions.

## W

**Waterfall**: A sequential design process, often used in software development, where progress flows steadily downward through phases.

**White Box Testing**: Testing based on an analysis of the internal structure of the component or system.

## Z

**Zero-Shot Learning**: An LLM's ability to perform tasks it wasn't explicitly trained for, without any examples provided in the prompt.

---

*This glossary provides definitions for key terms used in software quality and development practices, including AI/ML terminology. For additional terms, refer to industry standards and frameworks.*

**Recent Additions:**
- AI & Machine Learning terms (added October 2024)
- LLM and RAG terminology
- Prompt engineering concepts