# Glossary of Software Quality Terms

## A

**Acceptance Criteria**: Specific conditions that must be met for a user story to be considered complete and acceptable to the product owner.

**Agile**: An iterative approach to software development that emphasizes collaboration, customer feedback, and small, rapid releases.

**API (Application Programming Interface)**: A set of protocols, routines, and tools for building software applications that specify how software components should interact.

**Automated Testing**: The practice of using software tools to execute tests automatically, compare actual outcomes with predicted outcomes, and report results.

## B

**BDD (Behavior-Driven Development)**: A software development approach that combines Test-Driven Development (TDD) with ideas from Domain-Driven Design and object-oriented analysis.

**Bug**: An error, flaw, or fault in a computer program or system that causes it to produce an incorrect or unexpected result.

**Build**: The process of converting source code files into standalone software artifacts that can be run on a computer.

**Burndown Chart**: A graphical representation of work left to do versus time, showing the progress of a team toward completing a sprint or project.

## C

**CI/CD (Continuous Integration/Continuous Deployment)**: A method to frequently deliver apps to customers by introducing automation into the stages of app development.

**Code Coverage**: A measure of how much source code is tested by a particular test suite, usually expressed as a percentage.

**Code Review**: A systematic examination of computer source code intended to find bugs and improve code quality.

**Continuous Integration**: The practice of merging all developer working copies to a shared mainline several times a day.

**Cyclomatic Complexity**: A software metric used to indicate the complexity of a program by measuring the number of linearly independent paths through source code.

## D

**Definition of Done (DoD)**: A shared understanding of expectations that software must live up to in order to be releasable into production.

**Definition of Ready (DoR)**: A shared understanding of what makes a user story ready for development to begin.

**DORA Metrics**: Four key metrics for measuring DevOps performance: Deployment Frequency, Lead Time for Changes, Mean Time to Recovery, and Change Failure Rate.

**Deployment**: The process of installing, configuring, and enabling a specific application or set of applications.

## E

**End-to-End Testing (E2E)**: A methodology that tests an application's workflow from beginning to end to ensure the application works as expected.

**Epic**: A large body of work that can be broken down into a number of smaller stories or requirements.

**Estimation**: The process of predicting the effort required to implement a user story or feature.

## F

**Feature Flag**: A software development technique that turns certain functionality on and off during runtime, without deploying new code.

**Functional Testing**: Testing that validates the software system against the functional requirements/specifications.

## G

**GitFlow**: A branching model for Git that defines a strict branching structure designed around the project release.

**Given-When-Then**: A style of representing tests or acceptance criteria in a structured format for Behavior-Driven Development.

## I

**Integration Testing**: Testing performed to expose defects in the interfaces and interaction between integrated components.

**INVEST**: An acronym that defines criteria for good user stories: Independent, Negotiable, Valuable, Estimable, Small, Testable.

**Issue Tracking**: The process of tracking and managing software bugs, feature requests, and other issues throughout development.

## L

**Lead Time**: The time elapsed between when a request is made and when it is fulfilled.

**Load Testing**: Testing conducted to understand the behavior of the system under a specific expected load.

## M

**Mean Time to Recovery (MTTR)**: The average time required to repair a failed component or device.

**Metrics**: Quantitative measures used to track and assess the status of specific processes.

**Minimum Viable Product (MVP)**: A development technique where a new product is developed with sufficient features to satisfy early adopters.

**Mock**: A simulated object that mimics the behavior of real objects in controlled ways for testing purposes.

## N

**Non-Functional Requirements**: Requirements that specify criteria for the operation of a system, rather than specific behaviors.

## P

**Pair Programming**: A software development technique where two programmers work together at one workstation.

**Performance Testing**: Testing conducted to evaluate the speed, responsiveness, and stability of a computer system under a workload.

**Product Backlog**: An ordered list of features, functions, requirements, enhancements, and fixes that serves as the input for the sprint backlog.

## Q

**Quality Assurance (QA)**: A way of preventing mistakes and defects in manufactured products and avoiding problems when delivering products or services.

**Quality Gate**: A checkpoint in the software development process where quality criteria must be met before proceeding.

## R

**Refactoring**: The process of restructuring existing computer code without changing its external behavior.

**Regression Testing**: Re-running functional and non-functional tests to ensure that previously developed and tested software still performs correctly.

**Requirements Engineering**: The process of defining, documenting, and maintaining requirements in the engineering design process.

**Risk-Based Testing**: An approach to testing that prioritizes the testing of features and functions based on the risk of failure.

## S

**Scrum**: An agile framework for managing product development with an emphasis on software development.

**Security Testing**: Testing technique to determine whether a system protects data and maintains functionality as intended.

**Shift-Left**: The practice of moving testing, quality, and performance evaluation early in the development process.

**Shift-Right**: The practice of running tests in production-like environments to ensure quality in real-world conditions.

**SOLID Principles**: Five design principles intended to make software designs more understandable, flexible, and maintainable.

**Sprint**: A time-boxed iteration of a continuous development cycle within an Agile development methodology.

**Static Analysis**: Analysis of computer software performed without actually executing programs.

**Story Points**: A unit of measure for expressing an estimate of the overall effort required to fully implement a product backlog item.

## T

**Technical Debt**: The implied cost of additional rework caused by choosing an easy solution now instead of using a better approach.

**Test-Driven Development (TDD)**: A software development process that relies on the repetition of a very short development cycle.

**Test Automation**: The use of software separate from the software being tested to control the execution of tests.

**Test Case**: A set of conditions or variables under which a tester will determine whether a system under test satisfies requirements.

**Test Coverage**: A measure used to describe the degree to which the source code of a program is executed when a particular test suite runs.

**Traceability**: The ability to trace requirements forward to design and code and backward from code to requirements.

## U

**Unit Testing**: Testing of individual components or modules of a software application in isolation.

**User Acceptance Testing (UAT)**: Testing performed by end users to verify that a software system meets business requirements.

**User Story**: An informal, natural language description of features of a software system written from the perspective of an end user.

**Usability Testing**: Testing method to evaluate how easy user interfaces are to use.

## V

**Velocity**: A measure of the amount of work a team can tackle during a single sprint.

**Version Control**: A system that records changes to a file or set of files over time so that you can recall specific versions later.

**Vulnerability**: A weakness in a system that can be exploited by a threat to gain unauthorized access or perform unauthorized actions.

## W

**Waterfall**: A sequential design process, often used in software development, where progress flows steadily downward through phases.

**White Box Testing**: Testing based on an analysis of the internal structure of the component or system.

---

*This glossary provides definitions for key terms used in software quality and development practices. For additional terms, refer to industry standards and frameworks.*